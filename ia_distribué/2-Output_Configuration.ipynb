{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ced2e33",
   "metadata": {},
   "source": [
    "# LLM Output Configuration**\n",
    "**Modèle** : *Llama-3.2-1B (Instruct)* via **Ollama**  \n",
    "**Objectif** : Comprendre et maîtriser les paramètres qui contrôlent la **sortie** du modèle :\n",
    "- Output length  \n",
    "- Sampling controls  \n",
    "- Temperature  \n",
    "- Top‑K et Top‑P  \n",
    "- Putting it all together\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812224da",
   "metadata": {},
   "source": [
    "## Prérequis\n",
    "- Ollama lancé en local (serveur actif) et package Python `ollama` installé.  \n",
    "- Modèle : `llama3.2:1b` disponible.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f0f49cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optionnel) Installer/mettre à jour le client Python Ollama\n",
    "# !pip install -q --upgrade ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61b7c779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ollama est accessible en local.\n",
      "⏳ Vérification/téléchargement du modèle 'llama3.2:1b'...\n",
      "✅ Modèle prêt.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "def assert_ollama_ready():\n",
    "    try:\n",
    "        _ = ollama.list()\n",
    "        print(\"✅ Ollama est accessible en local.\")\n",
    "    except Exception as e:\n",
    "        raise SystemExit(\n",
    "            \"❌ Impossible de contacter Ollama. Démarrez le serveur (ex: 'ollama serve').\\n\"\n",
    "            f\"Détails: {e}\"\n",
    "        )\n",
    "\n",
    "assert_ollama_ready()\n",
    "\n",
    "try:\n",
    "    print(\"⏳ Vérification/téléchargement du modèle 'llama3.2:1b'...\")\n",
    "    ollama.pull('llama3.2:1b')\n",
    "    print(\"✅ Modèle prêt.\")\n",
    "except Exception as e:\n",
    "    print(\"ℹ️ Vérifiez le tag (ex: 'llama3.2:1b').\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bbe6eb",
   "metadata": {},
   "source": [
    "## Fonctions utilitaires\n",
    "Nous utilisons une fonction d’aide pour **lancer** des prompts avec différents paramètres et afficher un bref **récapitulatif**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56dcffc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Optional\n",
    "import textwrap\n",
    "\n",
    "MODEL = 'llama3.2:1b'\n",
    "\n",
    "def run(prompt: str, options: Optional[Dict] = None):\n",
    "    options = options or {}\n",
    "    r = ollama.chat(\n",
    "        model=MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        options=options\n",
    "    )\n",
    "    out = r[\"message\"][\"content\"]\n",
    "    return out\n",
    "\n",
    "def show(label: str, prompt: str, options: Dict):\n",
    "    print(\"\\n=== \" + label + \" ===\")\n",
    "    print(\"Options:\", {k: options[k] for k in sorted(options.keys())})\n",
    "    out = run(prompt, options)\n",
    "    print(\"\\nSortie:\\n\" + textwrap.fill(out, width=100))\n",
    "    print(f\"\\nLongueur: {len(out.split())} mots, {len(out)} caractères\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55a3058",
   "metadata": {},
   "source": [
    "## Prompt commun pour les expériences\n",
    "On utilisera des prompts courts pour visualiser l’impact des paramètres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17948971",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_SHORT = (\n",
    "    \"Explique en 2–3 phrases ce qu'est un modèle de langage, pour un public non technique.\"\n",
    ")\n",
    "PROMPT_CREATIVE = (\n",
    "    \"Écris une mini-description originale (2–3 phrases) d'un café de quartier un matin de pluie.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79be3793",
   "metadata": {},
   "source": [
    "## 1) Output length\n",
    "Le paramètre **`num_predict`** fixe le **nombre maximal de tokens générés** (le modèle peut s’arrêter plus tôt).  \n",
    "On peut aussi utiliser `stop` pour forcer un **arrêt** lorsqu’une séquence apparaît.\n",
    "\n",
    "**À retenir** :\n",
    "- `num_predict` = *plafond* de tokens générés (pas une garantie de longueur exacte).  \n",
    "- `stop` = *séquence d’arrêt* (ex. `\"\\n\\n\"`, `\"###\"`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c527a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== num_predict=32 (court) ===\n",
      "Options: {'num_predict': 32, 'seed': 7, 'temperature': 0.2}\n",
      "\n",
      "Sortie:\n",
      "Un modèle de langage est une forme d'information qui permet à des ordinateurs de comprendre et de\n",
      "générer du texte ou des images en fonction\n",
      "\n",
      "Longueur: 25 mots, 140 caractères\n",
      "\n",
      "=== num_predict=200 (long) ===\n",
      "Options: {'num_predict': 200, 'seed': 7, 'temperature': 0.2}\n",
      "\n",
      "Sortie:\n",
      "Un modèle de langage est une forme d'information qui permet de communiquer des idées ou des\n",
      "informations de manière claire et précise, souvent utilisée dans les réseaux sociaux, les\n",
      "applications mobiles et les sites web. Il s'agit d'un ensemble de règles et de structures pour\n",
      "organiser et exprimer des concepts, ce qui en fait une forme de langage unique et spécifique.\n",
      "\n",
      "Longueur: 61 mots, 370 caractères\n",
      "\n",
      "=== stop=['FIN'] ===\n",
      "Options: {'num_predict': 200, 'seed': 7, 'stop': ['FIN'], 'temperature': 0.7}\n",
      "\n",
      "Sortie:\n",
      "Le café du quartier, un endroit de rêve à l'abri de la pluie, était silencieux et calme, les patrons\n",
      "assis dans le noir, les yeux fixés sur leurs cellules mobiles, les bruits de la pluie retentissant\n",
      "en bas. Le mélange de parfums et d'odeurs délicates entourait le café, une odeur réconfortante qui\n",
      "attirait tout le monde dans ce petit refuge de l'après-midi.\n",
      "\n",
      "Longueur: 62 mots, 359 caractères\n"
     ]
    }
   ],
   "source": [
    "show(\"num_predict=32 (court)\", PROMPT_SHORT, {\"num_predict\": 32, \"temperature\": 0.2, \"seed\": 7})\n",
    "show(\"num_predict=200 (long)\", PROMPT_SHORT, {\"num_predict\": 200, \"temperature\": 0.2, \"seed\": 7})\n",
    "\n",
    "# Exemple d'arrêt sur séquence 'FIN' (si le modèle la produit)\n",
    "show(\"stop=['FIN']\", PROMPT_CREATIVE, {\"num_predict\": 200, \"temperature\": 0.7, \"stop\": [\"FIN\"], \"seed\": 7})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a76f497",
   "metadata": {},
   "source": [
    "## 2) Sampling controls (panorama)\n",
    "Les **contrôles d’échantillonnage** déterminent *comment* le prochain token est choisi :\n",
    "\n",
    "- **Greedy** (pas d’échantillonnage) : `temperature=0` **et** `top_k=1` → le modèle prend toujours le meilleur token.  \n",
    "- **Sampling** : on *échantillonne* selon une distribution influencée par `temperature`, `top_k`, `top_p`.  \n",
    "- **Seed** : fixe la graine pseudo-aléatoire pour la **reproductibilité** partielle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3831fdcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Greedy (deterministe) ===\n",
      "Options: {'num_predict': 120, 'seed': 42, 'temperature': 0.0, 'top_k': 1}\n",
      "\n",
      "Sortie:\n",
      "Dans le quartier, les fenêtres éclairées par la lueur du matin se couvrent d'une ombre sombre,\n",
      "tandis que le bruit de la pluie frappe contre les toits et les murs. Le café, un refuge pour ceux\n",
      "qui cherchent à s'abriter, est rempli de gens qui discutent et rient dans le silence de la matinée\n",
      "pluvieuse. Les glaçons dans les tasses tremblent légèrement sous leurs mains, tandis que les aromas\n",
      "du café\n",
      "\n",
      "Longueur: 71 mots, 399 caractères\n",
      "\n",
      "=== Sampling (temp=0.7, top_k=40, top_p=0.9) ===\n",
      "Options: {'num_predict': 120, 'seed': 42, 'temperature': 0.7, 'top_k': 40, 'top_p': 0.9}\n",
      "\n",
      "Sortie:\n",
      "Le café de quartier, un endroit à l'ombre des arcades, se réveille lentement sous la pluie\n",
      "fatiguante du matin. Les fenêtres éclairées par le soleil passé font des ombres mystérieuses sur les\n",
      "murs bruyants, tandis que les patrons et les clients s'étirent avec déconfort pour trouver leurs\n",
      "chaises. La musique de jazz pèlerin vient d'une petite télévision installée dans une table à\n",
      "l'arrière, lançant un refrain mélancolique qui suscite\n",
      "\n",
      "Longueur: 69 mots, 436 caractères\n"
     ]
    }
   ],
   "source": [
    "show(\"Greedy (deterministe)\", PROMPT_CREATIVE, {\"temperature\": 0.0, \"top_k\": 1, \"num_predict\": 120, \"seed\": 42})\n",
    "show(\"Sampling (temp=0.7, top_k=40, top_p=0.9)\", PROMPT_CREATIVE, {\"temperature\": 0.7, \"top_k\": 40, \"top_p\": 0.9, \"num_predict\": 120, \"seed\": 42})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ad0a7d",
   "metadata": {},
   "source": [
    "## 3) Temperature\n",
    "**`temperature`** étale ou concentre la distribution :  \n",
    "- **0.0–0.2** : sobre/prévisible  \n",
    "- **0.3–0.7** : équilibré  \n",
    "- **0.8–1.2** : créatif/variable\n",
    "\n",
    "On garde `seed` constant pour mieux comparer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a0e38aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== temperature=0.0 ===\n",
      "Options: {'num_predict': 120, 'seed': 123, 'temperature': 0.0, 'top_k': 40, 'top_p': 0.9}\n",
      "\n",
      "Sortie:\n",
      "Dans le quartier, les fenêtres éclairées par la lueur du matin se couvrent d'une ombre sombre,\n",
      "tandis que le bruit de la pluie frappe contre les toits et les murs. Le café, un refuge pour ceux\n",
      "qui cherchent à s'abriter, est rempli de gens qui discutent et rient dans le silence de la matinée\n",
      "pluvieuse. Les glaçons dans les tasses tremblent légèrement sous leurs mains, tandis que les aromas\n",
      "du café\n",
      "\n",
      "Longueur: 71 mots, 399 caractères\n",
      "\n",
      "=== temperature=0.3 ===\n",
      "Options: {'num_predict': 120, 'seed': 123, 'temperature': 0.3, 'top_k': 40, 'top_p': 0.9}\n",
      "\n",
      "Sortie:\n",
      "Le café de quartier, un endroit secret où les gens se réunissent pour partager des conversations et\n",
      "des rires malgré la pluie qui tombe à l'ouest. Les fenêtres éclairées par le soleil couchant créent\n",
      "une lumière douce qui accueille les clients dans son intérieur chaud et confortable.\n",
      "\n",
      "Longueur: 47 mots, 284 caractères\n",
      "\n",
      "=== temperature=0.7 ===\n",
      "Options: {'num_predict': 120, 'seed': 123, 'temperature': 0.7, 'top_k': 40, 'top_p': 0.9}\n",
      "\n",
      "Sortie:\n",
      "Le café de quartier se réveillait sous la pluie, son toit en forme de croix de bois recouvert d'une\n",
      "laine épaisse, et les fenêtres éclairées par des lueurs sombres qui semblaient sortir du ciel. Les\n",
      "clients, assis à leurs tables rapprochées, regardaient le monde plouffler sous eux avec un mélange\n",
      "de curiosité et d'appréhension, comme si la pluie était un malentendu qui allait prendre tout\n",
      "\n",
      "Longueur: 65 mots, 391 caractères\n",
      "\n",
      "=== temperature=1.0 ===\n",
      "Options: {'num_predict': 120, 'seed': 123, 'temperature': 1.0, 'top_k': 40, 'top_p': 0.9}\n",
      "\n",
      "Sortie:\n",
      "Le café de quartier était désert, les fenêtres ensoleillées refletsant le ciel pluvieux comme des\n",
      "scènes de peinture verte. Le bruit doux du réverbération résonnait dans les rues, tandis que la\n",
      "pluie pétillait au coin du trottoir, formant des gouttes éclatantes qui tombaient sur le pavé avec\n",
      "un petit choc cristallin. Le parfum des croissants et de la glace dégustés à l'ext\n",
      "\n",
      "Longueur: 62 mots, 375 caractères\n"
     ]
    }
   ],
   "source": [
    "for t in [0.0, 0.3, 0.7, 1.0]:\n",
    "    show(f\"temperature={t}\", PROMPT_CREATIVE, {\"temperature\": t, \"top_k\": 40, \"top_p\": 0.9, \"num_predict\": 120, \"seed\": 123})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5f4ff2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fcff2d48",
   "metadata": {},
   "source": [
    "## 4) Top‑K et Top‑P\n",
    "- **Top‑K** : on restreint la **fenêtre** aux *K* meilleurs tokens.  \n",
    "- **Top‑P** (*nucleus*) : on inclut les meilleurs tokens jusqu’à atteindre une **masse de probabilité** *P*.  \n",
    "\n",
    "**Effets typiques** :\n",
    "- *K petit* ou *P faible* → sorties plus **conservatrices**, parfois répétitives.  \n",
    "- *K grand* ou *P haut* → plus de **diversité**, risque d’incohérence si trop élevé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d207c8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(\"top_k=10 (conservateur)\", PROMPT_CREATIVE, {\"temperature\": 0.7, \"top_k\": 10, \"top_p\": 0.95, \"num_predict\": 120, \"seed\": 9})\n",
    "show(\"top_k=1000 (large)\", PROMPT_CREATIVE, {\"temperature\": 0.7, \"top_k\": 1000, \"top_p\": 0.95, \"num_predict\": 120, \"seed\": 9})\n",
    "\n",
    "show(\"top_p=0.3 (nucleus serré)\", PROMPT_CREATIVE, {\"temperature\": 0.7, \"top_k\": 100, \"top_p\": 0.3, \"num_predict\": 120, \"seed\": 9})\n",
    "show(\"top_p=1.0 (quasi complet)\", PROMPT_CREATIVE, {\"temperature\": 0.7, \"top_k\": 100, \"top_p\": 1.0, \"num_predict\": 120, \"seed\": 9})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fade1d",
   "metadata": {},
   "source": [
    "### Température et distribution de probabilités\n",
    "\n",
    "Le modèle prédit pour chaque token possible une **logit** (valeur brute).  \n",
    "Ces logits passent ensuite par une fonction *softmax* pour donner une distribution de probabilités :\n",
    "\n",
    "\n",
    "![alt text](image1.png \"Title\")\n",
    "\n",
    "\n",
    "\n",
    "- **T = 1** → aucune modification.  \n",
    "- **T < 1** → les différences entre probabilités sont **amplifiées** (plus déterministe).  \n",
    "- **T > 1** → les différences sont **aplaties** (plus aléatoire).  \n",
    "\n",
    "---\n",
    "\n",
    "### Exemple\n",
    "\n",
    "Supposons que le modèle produise les logits suivants :  \n",
    "- \"chat\" : 5.0  \n",
    "- \"chien\" : 4.0  \n",
    "- \"dragon\" : 2.0  \n",
    "\n",
    "Après softmax :\n",
    "\n",
    "- **T = 1** → chat ≈ 70%, chien ≈ 25%, dragon ≈ 5%  \n",
    "- **T = 0.5** → chat ≈ 90%, chien ≈ 9%, dragon ≈ 1%  \n",
    "- **T = 2.0** → chat ≈ 50%, chien ≈ 30%, dragon ≈ 20%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7072de49",
   "metadata": {},
   "source": [
    "## 5) Putting it all together\n",
    "Des **presets** selon l’objectif :\n",
    "- **Deterministic QA** : réponses stables pour tests/CI.\n",
    "- **Balanced Summary** : équilibre entre concision et variation.\n",
    "- **Creative Rewrite** : plus de liberté stylistique.\n",
    "- **Short Answer** : contraintes fortes sur la longueur.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "524f7c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== preset: deterministic_qa ===\n",
      "Options: {'num_predict': 160, 'seed': 1234, 'temperature': 0.0, 'top_k': 1, 'top_p': 1.0}\n",
      "\n",
      "Sortie:\n",
      "L'IA générative est conçue pour créer de nouvelles données ou modèles, tandis que l'IA\n",
      "discriminative est conçue pour prédire des résultats basés sur des données existantes.\n",
      "\n",
      "Longueur: 26 mots, 173 caractères\n",
      "\n",
      "=== preset: balanced_summary ===\n",
      "Options: {'num_predict': 200, 'seed': 2024, 'temperature': 0.3, 'top_k': 40, 'top_p': 0.9}\n",
      "\n",
      "Sortie:\n",
      "Voici un résumé du texte en 4 points courtes :  - L'IA générative produit de nouveaux contenus à\n",
      "partir de modèles apprenant. - Cela implique la production de contenu à partir d'un modèle qui a\n",
      "appris sur une distribution spécifique. - L'IA discriminative, quant à elle, classe ou prédit des\n",
      "labels. - Les deux techniques se complètent dans des pipelines modernes pour générer et classifier\n",
      "du contenu.\n",
      "\n",
      "Longueur: 67 mots, 402 caractères\n",
      "\n",
      "=== preset: creative_rewrite ===\n",
      "Options: {'num_predict': 220, 'seed': 7, 'temperature': 0.9, 'top_k': 100, 'top_p': 0.95}\n",
      "\n",
      "Sortie:\n",
      "Dans l'écho de la pensée, une touche de grâce est ajoutée !   'Un modélus fournit un outil précieux\n",
      "pour construire des idées en un temps record.'\n",
      "\n",
      "Longueur: 26 mots, 146 caractères\n",
      "\n",
      "=== preset: short_answer ===\n",
      "Options: {'num_predict': 40, 'seed': 77, 'temperature': 0.2, 'top_k': 20, 'top_p': 0.8}\n",
      "\n",
      "Sortie:\n",
      "Choisissez des mots clés pertinents et précis dans le titre du prompt afin d'obtenir les résultats\n",
      "les plus utiles.\n",
      "\n",
      "Longueur: 19 mots, 115 caractères\n"
     ]
    }
   ],
   "source": [
    "PRESETS = {\n",
    "    \"deterministic_qa\": {\"temperature\": 0.0, \"top_k\": 1, \"top_p\": 1.0, \"num_predict\": 160, \"seed\": 1234},\n",
    "    \"balanced_summary\": {\"temperature\": 0.3, \"top_k\": 40, \"top_p\": 0.9, \"num_predict\": 200, \"seed\": 2024},\n",
    "    \"creative_rewrite\": {\"temperature\": 0.9, \"top_k\": 100, \"top_p\": 0.95, \"num_predict\": 220, \"seed\": 7},\n",
    "    \"short_answer\": {\"temperature\": 0.2, \"top_k\": 20, \"top_p\": 0.8, \"num_predict\": 40, \"seed\": 77},\n",
    "}\n",
    "\n",
    "PROMPT_QA = \"Réponds en 3 phrases : quelle est la différence entre IA générative et IA discriminative ?\"\n",
    "PROMPT_SUMMARY = (\n",
    "    \"Résume ce texte en 4 puces courtes : L'IA générative produit de nouveaux contenus à partir de modèles apprenant\"\n",
    "    \" des distributions, tandis que l'IA discriminative classe ou prédit des labels. Les deux se complètent dans\"\n",
    "    \" des pipelines modernes.\"\n",
    ")\n",
    "PROMPT_REWRITE = \"Réécris cette phrase avec un ton poétique : 'Un modèle aide à rédiger des idées plus vite.'\"\n",
    "PROMPT_SHORT = \"En une phrase, donne un conseil pour améliorer un prompt.\"\n",
    "\n",
    "show(\"preset: deterministic_qa\", PROMPT_QA, PRESETS[\"deterministic_qa\"])\n",
    "show(\"preset: balanced_summary\", PROMPT_SUMMARY, PRESETS[\"balanced_summary\"])\n",
    "show(\"preset: creative_rewrite\", PROMPT_REWRITE, PRESETS[\"creative_rewrite\"])\n",
    "show(\"preset: short_answer\", PROMPT_SHORT, PRESETS[\"short_answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52648d8f",
   "metadata": {},
   "source": [
    "## Bonnes pratiques\n",
    "- Baissez la **température** pour des formats stricts (JSON/YAML).  \n",
    "- Ajustez **`num_predict`** selon la *longueur attendue* et coupez via `stop` si besoin.  \n",
    "- Évitez des valeurs extrêmes **top_k/top_p** qui dégradent la cohérence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450a3ff0",
   "metadata": {},
   "source": [
    "## À retenir\n",
    "- `num_predict` : longueur maximale de génération ; `stop` pour arrêter proprement.  \n",
    "- **Sampling controls** : `temperature`, `top_k`, `top_p` façonnent la diversité.  \n",
    "- **Presets** : partez d’un réglage type, puis affinez pour votre cas d’usage."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
