{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16eb32d4",
   "metadata": {},
   "source": [
    "\n",
    "# Introduction à l’IA Générative — **Phase 1 : Basic Prompt Structure**  \n",
    "**Modèle** : *Llama‑3.2‑1B (Instruct)* via **Ollama**  \n",
    "**Objectif** : Comprendre et appliquer la structure d’un *prompt* de base pour obtenir des réponses plus pertinentes.\n",
    "\n",
    "> Cette leçon se concentre uniquement sur la **structure du prompt**. Les rôles (system/user/assistant), la séparation data/instructions, le formatage, les few‑shots et les prompts complexes viendront dans les phases suivantes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e5bc42",
   "metadata": {},
   "source": [
    "\n",
    "## Prérequis\n",
    "- **Ollama** installé et lancé en local (serveur actif).\n",
    "- Le package Python `ollama` (client) pour appeler l'API locale.\n",
    "- Modèle : `llama3.2:1b` (compatible avec les machines modestes).\n",
    "\n",
    "> Si Ollama n’est pas lancé, démarrez-le d’abord. Ensuite, exécutez les cellules ci‑dessous.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02679bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# (Optionnel) Installer/mettre à jour le client Python Ollama\n",
    "# Décommentez si besoin :\n",
    "# !pip install -q --upgrade ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc1cb278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ollama est accessible en local.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import ollama\n",
    "\n",
    "def assert_ollama_ready():\n",
    "    try:\n",
    "        _ = ollama.list()\n",
    "        print(\"✅ Ollama est accessible en local.\")\n",
    "    except Exception as e:\n",
    "        raise SystemExit(\n",
    "            \"❌ Impossible de contacter Ollama. Assurez-vous que le serveur tourne (ex: 'ollama serve').\\n\"\n",
    "            f\"Détails: {e}\"\n",
    "        )\n",
    "\n",
    "assert_ollama_ready()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1853ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"⏳ Vérification/téléchargement du modèle 'llama3.2:1b'...\")\n",
    "    ollama.pull('llama3.2:1b')\n",
    "    print(\"✅ Modèle prêt.\")\n",
    "except Exception as e:\n",
    "    print(\"ℹ️ Si le pull échoue, vérifiez le tag de modèle (ex: 'llama3.2:1b' ou 'llama3.2:1b-instruct').\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dd749e",
   "metadata": {},
   "source": [
    "\n",
    "## Exemple minimal : un prompt simple\n",
    "Le prompt est juste une **instruction** claire. On commence par la version la plus simple possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96cdfe0",
   "metadata": {},
   "source": [
    "# Exemple Via Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6907e26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers accelerate torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b4d0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\salah\\anaconda3\\envs\\ragllm\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"hf_huggingfaceapi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3620b45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse : Définis l'intelligence artificielle en une phrase. La nouvelle Intelligence Artificielle, ou IA, est le résultat de la fusion entre la science et l'art. Les plus grands scientifiques et ingénieurs du monde entier se sont unis pour créer l'IA. L'IA peut être vue comme un outil de révolution qui permettra à la société de prendre un pas en avant dans le domaine de l'intelligence artificielle. L'IA peut être utilisée pour créer de nouveaux services,\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)  \n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "\n",
    "prompt = \"Définis l'intelligence artificielle en une phrase.\"\n",
    "\n",
    "# Tokenisation\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Génération\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "\n",
    "# Décodage\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Réponse :\", response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52040a81",
   "metadata": {},
   "source": [
    "# Exemple via API :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ac5291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement de la clé api \n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "with open(\"config.json\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "api_key = config[\"OPENAI_API_KEY\"]\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "171e5afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse : L'intelligence artificielle est un domaine de l'informatique qui vise à créer des systèmes capables d'accomplir des tâches nécessitant normalement l'intelligence humaine, telles que la reconnaissance de la parole, la prise de décisions, et la compréhension du langage naturel.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Définis l'intelligence artificielle en une phrase.\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    ")\n",
    "\n",
    "print(\"Réponse :\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c043578b",
   "metadata": {},
   "source": [
    "\n",
    "## Anatomie d’un bon *prompt* (structure de base)\n",
    "Un *prompt* efficace comprend généralement :\n",
    "\n",
    "1. **Tâche/objectif clair** — ce que vous voulez que le modèle fasse.\n",
    "2. **Contexte** (facultatif mais utile) — ce que le modèle doit savoir pour mieux répondre.\n",
    "3. **Contraintes** — longueur, style, ton, niveau de détail, interdits, etc.\n",
    "4. **Format de sortie** (basique ici) — phrase, liste, étapes numérotées, etc.\n",
    "\n",
    "> **Règle d’or** : *clair, concret, contraint*. Évitez les formulations vagues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d30a6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ask(prompt: str, temperature: float = 0.2):\n",
    "    \n",
    "    \n",
    "    r = ollama.chat(\n",
    "        model='llama3.2:1b',\n",
    "        messages=[{'role': 'user', 'content': prompt}],\n",
    "        options={'temperature': temperature}\n",
    "    )\n",
    "    return r['message']['content']\n",
    "\n",
    "\n",
    "print(ask(\"En une phrase, explique la différence entre IA faible et IA forte.\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4dc932",
   "metadata": {},
   "source": [
    "\n",
    "## Démo : flou vs. précis\n",
    "Comparons un prompt **vague** à un prompt **précis** avec contraintes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a9890b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt_vague = \"Parle de l'IA.\"\n",
    "prompt_precis = (\n",
    "    \"Explique brièvement ce qu’est l’intelligence artificielle en **3 points numérotés** : \"\n",
    "    \"1) définition simple, 2) un exemple concret du quotidien, 3) une limite actuelle. \"\n",
    "    \"Utilise des phrases courtes.\"\n",
    ")\n",
    "\n",
    "print(\"— Prompt vague —\")\n",
    "print(prompt_vague)\n",
    "print(\"\\nRéponse :\\n\", ask(prompt_vague))\n",
    "\n",
    "print(\"\\n\\n— Prompt précis —\")\n",
    "print(prompt_precis)\n",
    "print(\"\\nRéponse :\\n\", ask(prompt_precis))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b481e2b2",
   "metadata": {},
   "source": [
    "\n",
    "## Modèle (template) de prompt de base\n",
    "Utilisez ce canevas et remplacez `{{...}}` :\n",
    "\n",
    "```\n",
    "TÂCHE : {{ce que je veux que le modèle fasse en une phrase}}\n",
    "\n",
    "CONTEXTE (optionnel) : {{faits, hypothèses, public visé, contraintes de domaine}}\n",
    "\n",
    "CONTRAINTES : \n",
    "- Longueur : {{ex. 100–150 mots / 3 points / une phrase}}\n",
    "- Style/ton : {{clair, pédagogique, professionnel, grand public…}}\n",
    "- Interdits : {{jargon inutile, spéculation, etc.}}\n",
    "\n",
    "FORMAT DE SORTIE : {{phrase, liste à puces, étapes numérotées, etc.}}\n",
    "```\n",
    "\n",
    "> Astuce : commencez simple, ajoutez des contraintes seulement si nécessaire.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
