{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63d00a7c",
   "metadata": {},
   "source": [
    "# Formatting Output and Speaking**\n",
    "**Modèle** : *Llama-3.2-1B (Instruct)* via **Ollama**  \n",
    "**Objectif** : **Contrôler le format de sortie** (listes, tableaux, JSON/YAML) et le **style d’expression** (ton, concision, voix) pour obtenir des réponses exploitables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7844ea",
   "metadata": {},
   "source": [
    "## Pourquoi le format & le style comptent ?\n",
    "- **Lisibilité** : pour un lecteur humain (listes, étapes, titres).  \n",
    "- **Exploitation** : pour une machine (JSON/YAML strict).  \n",
    "- **Constance** : même style/ton sur plusieurs réponses (voix de marque)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bc8504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optionnel) Installer/mettre à jour le client Python Ollama\n",
    "# !pip install -q --upgrade ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f62feb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ollama est accessible en local.\n",
      "⏳ Vérification/téléchargement du modèle 'llama3.2:1b'...\n",
      "✅ Modèle prêt.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "def assert_ollama_ready():\n",
    "    try:\n",
    "        _ = ollama.list()\n",
    "        print(\"✅ Ollama est accessible en local.\")\n",
    "    except Exception as e:\n",
    "        raise SystemExit(\n",
    "            \"❌ Impossible de contacter Ollama. Démarrez le serveur (ex: 'ollama serve').\\n\"\n",
    "            f\"Détails: {e}\"\n",
    "        )\n",
    "\n",
    "assert_ollama_ready()\n",
    "\n",
    "try:\n",
    "    print(\"⏳ Vérification/téléchargement du modèle 'llama3.2:1b'...\")\n",
    "    ollama.pull('llama3.2:1b')\n",
    "    print(\"✅ Modèle prêt.\")\n",
    "except Exception as e:\n",
    "    print(\"ℹ️ Vérifiez le tag (ex: 'llama3.2:1b').\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17737d81",
   "metadata": {},
   "source": [
    "## Fonctions utilitaires\n",
    "- `chat_fmt(system, user, temperature)` : gère un **system prompt** pour imposer ton & format.  \n",
    "- `expect_json(system, user, temperature)` : vérifie que la sortie est un **JSON** valide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb10df6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import json\n",
    "\n",
    "def chat_fmt(system_prompt: str, user_prompt: str, temperature: float = 0.2) -> str:\n",
    "    messages = []\n",
    "    if system_prompt:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "    r = ollama.chat(model=\"llama3.2:1b\", messages=messages, options={\"temperature\": temperature})\n",
    "    return r[\"message\"][\"content\"]\n",
    "\n",
    "def expect_json(system_prompt: str, user_prompt: str, temperature: float = 0.0):\n",
    "    out = chat_fmt(system_prompt, user_prompt, temperature)\n",
    "    print(out)\n",
    "    try:\n",
    "        obj = json.loads(out)\n",
    "        print(\"\\n✅ JSON valide — clés:\", list(obj.keys()))\n",
    "    except Exception as e:\n",
    "        print(\"\\n❌ Sortie non JSON ou invalide:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1fa4b6",
   "metadata": {},
   "source": [
    "## 1) Listes & étapes numérotées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f817efdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voici quelques bonnes pratiques pour rédiger un prompt efficace :\n",
      "\n",
      "*   **Clé de recherche** : identifiez les mots clés et les concepts qui seront abordés dans le prompt.\n",
      "*   **Objectif** : définissez l'objectif du prompt, c'est-à-dire ce que vous souhaitez obtenir ou explorer.\n",
      "*   **Contenu** : résumez brièvement le contenu de votre document ou de votre projet.\n",
      "*   **Format** : indiquez si vous avez besoin d'un format spécifique (par exemple, PDF, Word, etc.).\n",
      "*   **Durée** : précisez la durée de l'activité ou du projet.\n",
      "*   **Références** : mentionnez les sources ou les ressources qui seront utilisées dans le prompt.\n"
     ]
    }
   ],
   "source": [
    "system_list = (\n",
    "    \"Tu es un formateur. Réponds **uniquement** par une liste à puces (3 à 5 puces), phrases courtes.\"\n",
    ")\n",
    "user_q = \"Donne des bonnes pratiques pour rédiger un prompt efficace.\"\n",
    "print(chat_fmt(system_list, user_q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e5eb29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voici les 5 étapes pour tester un prompt et l'améliorer rapidement :\n",
      "\n",
      "1. **Définir le but de la modification** : Avant de commencer à modifier le prompt, identifiez clairement ce que vous voulez accomplir. Quels sont vos objectifs ? Quel est votre but principal ? Cela vous aidera à rester concentré et à prendre des décisions éclairées.\n",
      "\n",
      "2. **Analyser la fonctionnalité** : Examine attentivement le comportement du prompt actuel. Quels sont ses besoins spécifiques ? Quelles actions souhaitez-vous que le user effectue pour répondre à son besoin ? Cela vous aidera à identifier les points faibles et les opportunités d'amélioration.\n",
      "\n",
      "3. **Identifiez les problèmes** : Évaluez les problèmes qui existent actuellement avec le prompt. Quels sont les erreurs, les limites ou les difficultés que les utilisateurs rencontrent ? Cela vous aidera à identifier les points de faiblesse et à trouver des solutions pour les résoudre.\n",
      "\n",
      "4. **Proposer des alternatives** : Créez des alternatives pour résoudre les problèmes identifiés. Quels sont les options alternatives qui pourraient être plus efficaces ou plus utiles ? Cela vous aidera à améliorer la fonctionnalité du prompt et à réduire le nombre de erreurs.\n",
      "\n",
      "5. **Testez et ajustez** : Testez les modifications proposées avec un groupe d'utilisateurs ou des utilisateurs réels. Ajustez les options pour obtenir les résultats souhaités. Cela vous aidera à valider vos décisions et à améliorer la fonctionnalité du prompt de manière efficace.\n"
     ]
    }
   ],
   "source": [
    "system_steps = (\n",
    "    \"Tu es un coach. Réponds en **5 étapes numérotées** (format '1. ...').\"\n",
    ")\n",
    "user_q = \"Explique comment tester un prompt et l'améliorer rapidement.\"\n",
    "print(chat_fmt(system_steps, user_q))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb94e31",
   "metadata": {},
   "source": [
    "## 2) Tableaux Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb111821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Critère | IA Générative (A) | IA Symbolique (B) |\n",
      "| --- | --- | --- |\n",
      "| **Complexité** | Plus complexe, nécessite des connaissances plus vives en informatique et en logiciel | Plus simple, nécessite moins de connaissances en informatique et en logiciel |\n",
      "| **Applicabilité** | Plus adaptée aux tâches répétitives et régulières | Plus adaptable aux tâches complexes et à l'analyse de données |\n",
      "| **Intelligence Artificielle** | Nécessite des connaissances plus vives en intelligence artificielle pour optimiser les processus | Nécessite moins de connaissances en intelligence artificielle, mais peut être utilisée pour certaines tâches |\n",
      "| **Utilisation** | Typiquement utilisée dans les applications de gestion de projet et de production | Typiquement utilisée dans les applications de traitement de données et d'analyse de données |\n"
     ]
    }
   ],
   "source": [
    "system_table = (\n",
    "    \"Tu es un rédacteur technique. Réponds **uniquement** avec un tableau Markdown à 3 colonnes : \"\n",
    "    \"| Critère | Option A | Option B | et **aucun autre texte**.\"\n",
    ")\n",
    "user_q = \"Compare 'IA générative' (A) et 'IA symbolique' (B) sur 4 critères au choix.\"\n",
    "print(chat_fmt(system_table, user_q))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9a13bf",
   "metadata": {},
   "source": [
    "## 3) JSON strict (machine-consommable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "860b9514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ \"concepts\": [\"prompt\", \"few_shot\"], \"definition_courte\": \"\", \"niveau\": \"debutant\" }\n",
      "\n",
      "✅ JSON valide — clés: ['concepts', 'definition_courte', 'niveau']\n"
     ]
    }
   ],
   "source": [
    "system_json = (\n",
    "    \"Tu es un générateur d'outputs machine-consommables. \"\n",
    "    \"La réponse doit être **uniquement** un objet JSON valide, sans texte autour.\"\n",
    ")\n",
    "\n",
    "system_json2 = (\n",
    "    \"Tu es un générateur d'outputs machine-consommables. \"\n",
    "    \"Réponds avec un **objet JSON valide** UNIQUEMENT, sans Markdown, sans texte autour. \"\n",
    "    \"Commence par { et termine par }.\"\n",
    ")\n",
    "user_q = (\n",
    "    \"Retourne un JSON avec: {\"\n",
    "    \"\\\"concepts\\\":[\\\"prompt\\\",\\\"few_shot\\\"],\"\n",
    "    \"\\\"definition_courte\\\":\\\"\\\",\"\n",
    "    \"\\\"niveau\\\":\\\"debutant\\\"}\"\n",
    ")\n",
    "expect_json(system_json2, user_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b817de4f",
   "metadata": {},
   "source": [
    "## 4) YAML (lisible humain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7294f350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```yml\n",
      "prompt:\n",
      "  type: evaluation\n",
      "  description: Évaluation du prompt\n",
      "  task: task\n",
      "  name: évaluation-prompt\n",
      "  parameters:\n",
      "    - name: task_name\n",
      "      type: string\n",
      "      default: \"évaluation-prompt\"\n",
      "    - name: metrics\n",
      "      type: list\n",
      "      items:\n",
      "        - name: clarity\n",
      "          type: float\n",
      "        - name: faithfulness\n",
      "          type: float\n",
      "    - name: max_tokens\n",
      "      type: integer\n",
      "      default: 128\n",
      "```\n",
      "\n",
      "Ce fichier YAML définit une ébauche de configuration pour une évaluation de prompt. Il contient les champs suivants :\n",
      "\n",
      "- `task`: défini comme étant un \"task\" (un type d'évaluation)\n",
      "- `name`: le nom du task, qui peut être défini par l'utilisateur\n",
      "- `parameters`: un tableau de paramètres pour le task\n",
      "  - `task_name` : le nom du task, qui peut être défini par l'utilisateur\n",
      "  - `metrics` : une liste de champs de mesure (clarity et faithfulness) qui seront utilisés pour l'évaluation\n",
      "    - `clarity` : un champ de mesure float qui représente la clarté du prompt\n",
      "    - `faithfulness` : un champ de mesure float qui représente la fiabilité du prompt\n"
     ]
    }
   ],
   "source": [
    "system_yaml = (\n",
    "    \"Tu es un assistant technique. Réponds **uniquement** en YAML valide.\"\n",
    ")\n",
    "user_q = (\n",
    "    \"Donne une ébauche de config pour une évaluation de prompt avec champs:\"\n",
    "    \" task: nom, metrics: [clarity, faithfulness], max_tokens: 128\"\n",
    ")\n",
    "print(chat_fmt(system_yaml, user_q))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb10950",
   "metadata": {},
   "source": [
    "## 5) Ton & voix (speaking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27e187b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'IA générative et l'IA discriminative sont deux types d'applications de l'intelligence artificielle (IA) qui utilisent différentes approches pour traiter les données.\n",
      "\n",
      "L'IA générative utilise des algorithmes pour créer des modèles ou des réseaux de neurones basés sur les données. Elle est conçue pour analyser et comprendre le comportement des données, mais elle ne prend pas de décision basée sur ces analyses.\n",
      "\n",
      "En revanche, l'IA discriminative utilise des algorithmes pour prédire une variable ou un attribut en fonction d'une variable ou d'un attribut. Elle prend donc des décisions basées sur les analyses et les modèles générés par la génération de données.\n"
     ]
    }
   ],
   "source": [
    "system_formel = (\n",
    "    \"Tu es un rédacteur formel et précis. Style neutre, sans métaphores. 80 mots max.\"\n",
    ")\n",
    "user_q = \"Explique la différence entre IA générative et IA discriminative.\"\n",
    "print(chat_fmt(system_formel, user_q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2f64335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour ! 🤗\n",
      "\n",
      "L'IA générative et l'IA discriminative sont deux types d'applications de l'intelligence artificielle (IA) qui utilisent différentes approches pour traiter les données.\n",
      "\n",
      "L'IA discriminative est une approche qui cherche à prédire un résultat ou une décision basée sur des caractéristiques spécifiques des données. Elle utilise souvent des modèles de réseaux neuronaux et des algorithmes de classification pour faire cela.\n",
      "\n",
      "L'IA générative, en revanche, est une approche qui cherche à créer quelque chose (par exemple, un modèle de prévision ou un graphique) basée sur les données. Elle utilise souvent des modèles de réseaux neuronaux et des algorithmes de génération pour créer ce que l'on veut.\n",
      "\n",
      "En somme, l'IA discriminative est comme un photographe qui prend une photo d'un objet, tandis que l'IA générative est comme un artiste qui crée une œuvre d'art basée sur les données ! 🎨📸\n"
     ]
    }
   ],
   "source": [
    "system_friendly = (\n",
    "    \"Tu es un coach amical. Ton chaleureux, phrases courtes, émojis permis (max 2). 70 mots max.\"\n",
    ")\n",
    "user_q = \"Explique la différence entre IA générative et IA discriminative.\"\n",
    "print(chat_fmt(system_friendly, user_q))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1d7bf7",
   "metadata": {},
   "source": [
    "## 6) Contrôle de longueur & verbiage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d8a71d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'embedding est un processus dans l'intelligence artificielle (IA) qui consiste à représenter des données non structurées sous forme de vecteurs ou d'espaces numériques, permettant ainsi une représentation et une analyse plus efficaces.\n"
     ]
    }
   ],
   "source": [
    "system_len = (\n",
    "    \"Tu es concis. Réponds en **≤ 50 mots**.\"\n",
    ")\n",
    "user_q = \"Donne une définition simple de 'embedding' en IA.\"\n",
    "print(chat_fmt(system_len, user_q))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de3b04b",
   "metadata": {},
   "source": [
    "## 7) Blocs de code & pseudo-citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a6d1bca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemple de pseudo-code pour appeler une API de LLM en Python :\n",
      "```python\n",
      "import requests\n",
      "\n",
      "def get_llm_api(url: str, api_key: str) -> dict:\n",
      "    \"\"\"\n",
      "    Appelle l'API de LLM et récupère les résultats.\n",
      "    \n",
      "    Args:\n",
      "        url (str): L'adresse URL de l'API.\n",
      "        api_key (str): Le clé d'accès à l'API.\n",
      "    \n",
      "    Returns:\n",
      "        dict: Les résultats de la requête.\n",
      "    \"\"\"\n",
      "    headers = {'Authorization': f'Bearer {api_key}'}\n",
      "    response = requests.get(url, headers=headers)\n",
      "    return response.json()\n",
      "```\n",
      "Exemple de référence générale :\n",
      "```python\n",
      "# Récupère les informations de l'utilisateur\n",
      "user_id = 12345\n",
      "\n",
      "# Appelle l'API de LLM pour obtenir des réponses\n",
      "llm_api_response = get_llm_api('https://api.llm.com/v1/users', 'YOUR_API_KEY')\n",
      "\n",
      "# Affiche les résultats\n",
      "print(llm_api_response)\n",
      "```\n",
      "Références générales :\n",
      "```python\n",
      "# Récupère les informations de l'utilisateur et affiche les résultats\n",
      "def get_user_info(user_id: int, api_key: str) -> dict:\n",
      "    \"\"\"\n",
      "    Récupère les informations d'un utilisateur et les affiche.\n",
      "    \n",
      "    Args:\n",
      "        user_id (int): L'ID du'utilisateur.\n",
      "        api_key (str): La clé d'accès à l'API.\n",
      "    \n",
      "    Returns:\n",
      "        dict: Les informations de l'utilisateur.\n",
      "    \"\"\"\n",
      "    url = f'https://api.llm.com/v1/users/{user_id}'\n",
      "    return get_llm_api(url, api_key)\n",
      "\n",
      "# Récupère les réponses de l'API de LLM pour un utilisateur spécifique\n",
      "llm_user_response = get_user_info(user_id=12345, api_key='YOUR_API_KEY')\n",
      "print(llm_user_response)\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "system_code = (\n",
    "    \"Tu es un instructeur. Réponds avec: 1) un bloc de code Markdown Python minimal, 2) une liste de 2 références au format [ref].\"\n",
    ")\n",
    "user_q = \"Montre un exemple d'appel à une API de LLM (pseudo-code) et ajoute 2 références générales.\"\n",
    "print(chat_fmt(system_code, user_q))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c14e24c",
   "metadata": {},
   "source": [
    "## 8) Indices pour fiabiliser les formats\n",
    "- Rappeler **dans `system`** : « **Uniquement** JSON/YAML/Tableau Markdown, sans texte autour ».  \n",
    "- **Température basse** (`0.0–0.2`) pour formats stables.  \n",
    "- Ajouter une **stratégie de fallback** : `null` si info manquante."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbcca7d",
   "metadata": {},
   "source": [
    "## Template de `system` pour format + ton\n",
    "Copiez/collez et adaptez :\n",
    "\n",
    "Tu es un <persona>. **Sortie uniquement au format <JSON/YAML/Markdown-table/puces/étapes>**.  \n",
    "Respecte strictement : <structure demandée>.  \n",
    "Ton : <formel/friendly/marketing/technique>.  \n",
    "Longueur : <N mots max>.  \n",
    "Si information manquante : indique `null`/\"inconnu\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497e1c31",
   "metadata": {},
   "source": [
    "## À retenir (Phase 4)\n",
    "- Le **format** (listes, tableaux, JSON/YAML) et le **style** sont pilotés par le rôle `system`.  \n",
    "- Utiliser des formulations strictes (« **uniquement** JSON », « **5 étapes** », « **≤ 50 mots** »).  \n",
    "- Température basse pour **stabilité** du format.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06134edf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
